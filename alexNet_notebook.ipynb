{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.image as mpimg\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Activation, Flatten, Convolution2D, MaxPooling2D, ZeroPadding2D\n",
    "from keras.utils import np_utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://www.cs.toronto.edu/~kriz/cifar-100-python.tar.gz\n",
      "169009152/169001437 [==============================] - 110s 1us/step\n"
     ]
    }
   ],
   "source": [
    "# Import CIFAR100 small classification dataset\n",
    "\n",
    "from keras.datasets import cifar100\n",
    "\n",
    "(Xtrain, ytrain), (Xtest, ytest) = cifar100.load_data(label_mode = 'fine')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining early parameters\n",
    "\n",
    "N, w, h, c = Xtrain.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/blagawuga/anaconda3/envs/py367/lib/python3.6/site-packages/sklearn/preprocessing/_encoders.py:363: FutureWarning: The handling of integer data will change in version 0.22. Currently, the categories are determined based on the range [0, max(values)], while in the future they will be determined based on the unique values.\n",
      "If you want the future behaviour and silence this warning, you can specify \"categories='auto'\".\n",
      "In case you used a LabelEncoder before this OneHotEncoder to convert the categories to integers, then you can now use the OneHotEncoder directly.\n",
      "  warnings.warn(msg, FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "# One hot encoding labels\n",
    "\n",
    "encoder = OneHotEncoder()\n",
    "ytrain_ind = encoder.fit_transform(ytrain).toarray()\n",
    "ytest_ind = encoder.transform(ytest).toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalize input data (NOTE: - Original AlexNet paper didn't implement input Normalization, rather used layer normalization)\n",
    "\n",
    "Xtrain = Xtrain/255.\n",
    "Xtest = Xtest/255."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# AlexNet model\n",
    "\n",
    "### __[Original AlexNet paper](https://papers.nips.cc/paper/4824-imagenet-classification-with-deep-convolutional-neural-networks)__\n",
    "\n",
    "<img src=\"https://neurohive.io/wp-content/uploads/2018/10/AlexNet-1.png\" alt=\"Model structure for AlexNet\" title=\"Layer design of AlexNet\" />"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Original AlexNet uses 224x224x3 RGB Images from ImageNet dataset, we'll be using 32x32x3 from CIFAR100 dataset\n",
    "\n",
    "class AlexNet:\n",
    "    \"\"\"\n",
    "        Sci-kit type based model for implementing AlexNet*\n",
    "    \"\"\"\n",
    "    def __init__(self, n_classes, kernel_sizes, fc_layer_sizes, *input_shape):\n",
    "        (self.N, self.w, self.h, self.c) = input_shape\n",
    "        self.n_class = n_classes\n",
    "        self.kernel_sizes = kernel_sizes\n",
    "        self.fc_layer_sizes = fc_layer_sizes\n",
    "        self.model = Sequential()\n",
    "        assert len(kernel_sizes) == 5 and type(kernel_sizes) == list\n",
    "        assert len(fc_layer_sizes) == 2 and type(fc_layer_sizes) == list\n",
    "    \n",
    "    def initialize_model(self):\n",
    "        self.model.add(Convolution2D(filters=96, input_shape=(self.w, self.h, self.c), kernel_size=self.kernel_sizes[0], strides=(4,4), activation='relu', padding='valid', bias_initializer='ones'))\n",
    "        self.model.add(MaxPooling2D(pool_size=(2, 2), strides=(1, 1)))\n",
    "        self.model.add(ZeroPadding2D(padding=(2, 2), data_format='channels_last'))\n",
    "        self.model.add(Convolution2D(filters=256, kernel_size=self.kernel_sizes[1], strides=(1,1), activation='relu', padding='valid', bias_initializer='ones'))\n",
    "        self.model.add(MaxPooling2D(pool_size=(2, 2), strides=(1, 1)))\n",
    "        self.model.add(ZeroPadding2D(padding=(1, 1), data_format='channels_last'))\n",
    "        self.model.add(Convolution2D(filters=384, kernel_size=self.kernel_sizes[2], strides=(1,1), activation='relu', padding='valid', bias_initializer='ones'))\n",
    "        self.model.add(ZeroPadding2D(padding=(1, 1), data_format='channels_last'))\n",
    "        self.model.add(Convolution2D(filters=384, kernel_size=self.kernel_sizes[3], strides=(1,1), activation='relu', padding='valid', bias_initializer='ones'))\n",
    "        self.model.add(ZeroPadding2D(padding=(1, 1), data_format='channels_last'))\n",
    "        self.model.add(Convolution2D(filters=256, kernel_size=self.kernel_sizes[3], strides=(1,1), activation='relu', padding='valid', bias_initializer='ones'))\n",
    "        self.model.add(MaxPooling2D(pool_size=(2, 2), strides=(1, 1)))\n",
    "        self.model.add(Flatten())\n",
    "        self.model.add(Dense(self.fc_layer_sizes[0]))\n",
    "        self.model.add(Dropout(rate=0.5))\n",
    "        self.model.add(Dense(self.fc_layer_sizes[1]))\n",
    "        self.model.add(Dropout(rate=0.5))\n",
    "        self.model.add(Dense(self.n_class, activation='softmax'))\n",
    "        self.model.compile(loss='categorical_crossentropy',\n",
    "              optimizer='adam',\n",
    "              metrics=['accuracy'])\n",
    "    \n",
    "    def train_model(self, X, y, batch_size, n_epochs, verbose=1):\n",
    "        self.model.fit(X, y, batch_size=batch_size, epochs=n_epochs, verbose=verbose)\n",
    "        \n",
    "    def eval_model(self, Xtest, ytest, verbose):\n",
    "        score = self.model.evaluate(Xtest, ytest, verbose=verbose)\n",
    "        return score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check score with CIFAR dataset\n",
    "\n",
    "alexNet = AlexNet(100, [(11, 11), (5, 5), (3, 3), (3, 3), (3, 3)], [4096, 4096], *Xtrain.shape)\n",
    "alexNet.initialize_model()\n",
    "alexNet.train_model(Xtrain, ytrain_ind, 128, 10, 1)\n",
    "score = alexNet.eval_model(Xtest, ytest_ind, verbose)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
